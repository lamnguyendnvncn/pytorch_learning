{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent and Training Pipeline for a quick example about linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.80000067\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314574\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# doing everything manually Linear Regression\n",
    "import numpy as np\n",
    "\n",
    "# f = w*x\n",
    "\n",
    "# f = 2*x\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# loss MSE loss function\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y) ** 2\n",
    "# gradient: dj/dw = 1/n * 2 * (w*x-y) * x = mean * 2 * (y_pred-y) * x\n",
    "\n",
    "\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2 * x, y_pred - y).mean()\n",
    "\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradient\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# use autograd for computing the gradient, others are manually\n",
    "import torch\n",
    "\n",
    "# f = w*x\n",
    "\n",
    "# f = 2*x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# loss MSE loss function\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradient = backward pass, then the grad will stored in w.grad()\n",
    "    l.backward()\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    # zero grad\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# only the prediction is manually, we will use loss function  and optimizer from pytorch\n",
    "\"\"\"\n",
    "Steps of training model in pytorch\n",
    "1. design model (input, output size, forward pass)\n",
    "2. construct loss and optimizer\n",
    "3. training loop\n",
    "    - forward pass: compute prediction\n",
    "    - backward pass: gradients\n",
    "    - update weight\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w*x\n",
    "\n",
    "# f = 2*x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()  # use the neural network mse loss\n",
    "optimizer = torch.optim.SGD(\n",
    "    [w], lr=learning_rate\n",
    ")  # use the sochastic gradient descent optimizer\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward()\n",
    "    # update weights - optimization\n",
    "    optimizer.step()\n",
    "    # still have to clear the gradient before continuing\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -1.201\n",
      "epoch 1: w = 3.421, loss = 41.18035889, \n",
      "epoch 11: w = 2.035, loss = 0.01383961, \n",
      "epoch 21: w = 2.007, loss = 0.00007626, \n",
      "epoch 31: w = 2.005, loss = 0.00003904, \n",
      "epoch 41: w = 2.004, loss = 0.00002125, \n",
      "epoch 51: w = 2.003, loss = 0.00001157, \n",
      "epoch 61: w = 2.002, loss = 0.00000630, \n",
      "epoch 71: w = 2.001, loss = 0.00000343, \n",
      "epoch 81: w = 2.001, loss = 0.00000187, \n",
      "epoch 91: w = 2.001, loss = 0.00000102, \n",
      "Prediction after training: f(5) = 10.001\n"
     ]
    }
   ],
   "source": [
    "# everything will use pytorch lib (forward, backward, optimizer)\n",
    "\"\"\"\n",
    "Steps of training model in pytorch\n",
    "1. design model (input, output size, forward pass)\n",
    "2. construct loss and optimizer\n",
    "3. training loop\n",
    "    - forward pass: compute prediction using linear model\n",
    "    - backward pass: gradients\n",
    "    - update weight\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w*x\n",
    "\n",
    "# f = 2*x\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "# how to define/create your own model with pytorch\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "# training\n",
    "learning_rate = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()  # use the neural network mse loss\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=learning_rate\n",
    ")  # use the sochastic gradient descent optimizer\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    # gradient = backward pass\n",
    "    l.backward()\n",
    "    # update weights - optimization\n",
    "    optimizer.step()\n",
    "    # still have to clear the gradient before continuing\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}, \")\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
